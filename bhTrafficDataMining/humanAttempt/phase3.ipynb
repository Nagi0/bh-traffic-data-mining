{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autores: Bruno Leal Fonseca & Guilherme Namen Pimenta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Understanding & Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A base de dados consiste em pastas contendo o mês e ano dos registros feitos pelos radares de Belo Horizonte. Cada mês contém subpastas para os respectivos dias, onde é possível encontrar conjuntos de arquivos no formato JSON. Nelas há registros de todos os radares. Um exemplo da estrutura de pastas é dado abaixo: \n",
    "\n",
    "ABRIL_2022/ \n",
    "\n",
    "┣ 20220401/ \n",
    "\n",
    "┃ ┣ 20220401_00.json \n",
    "\n",
    "┃ ┣ 20220401_01.json \n",
    "\n",
    "┃ ┣ 20220401_02.json \n",
    "\n",
    "┃ ┣ 20220401_03.json \n",
    "\n",
    "┃ ┣ 20220401_04.json \n",
    "\n",
    "┃ ┣ 20220401_05.json  \n",
    "\n",
    "Nota-se que cada arquivo JSON contem os dados de uma faixa de horário no respectivo dia.\n",
    "\n",
    "Uma das primeiras suposições é que se deve encontrar associações diretas entre as velocidades registradas com os horários, tipos de veículos e localização na via. Um determinado local pode ser mais comum encontrar velocidades mais altas em determinados horários ou em determinados tipos de veículos. Velocidades mais baixas ou altas também podem ser encontradas em horários bem comuns.Além do mais, pode-se encontrar padrões frequentes em casos que se registra infrações por excesso de velocidade.\n",
    "\n",
    "Foram adquiridos dados do ano de 2022 inteiro, mesclando todo os JSON em dataframes. Contudo o volume de dados é muito grande, portanto, usar biblioteca como pandas não será o suficiente. A solução foi usar um framework mais adequado para Big Data, o `Polars`. Uma vez que os dado estiverem processados, transformados e as informações necessárias filtradas, eles serão levados até a modelagem.\n",
    "\n",
    "Os atributos da base são as seguintes:\n",
    "- ID EQP: Número de identificação do equipamento no sistema de \n",
    "processamento. \n",
    "- DATA HORA: Data e hora de ocorrência do evento. \n",
    "- MILESEGUNDO: Milésimo de segundo de ocorrência do evento. \n",
    "- FAIXA: Faixa de circulação da via onde ocorreu o evento. \n",
    "- ID DE ENDEREÇO: Localização do equipamento no sistema de processamento. \n",
    "- VELOCIDADE DA VIA: Velocidade regulamentada para a via. \n",
    "- VELOCIDADE AFERIDA: Velocidade medida pelo equipamento. \n",
    "- CLASSIFICACAO: Tipo de veículo registrado pelo equipamento (Automóvel/Motocicleta/Caminhão ou ônibus). \n",
    "- TAMANHO: Comprimento do veículo detectado pelo equipamento. \n",
    "- NUM SERIE: Número de identificação do equipamento. \n",
    "- LATITUDE: Localização de coordenada geográfica latitude do equipamento. \n",
    "- LONGITUDE: Localização de coordenada geográfica longitude do equipamento. \n",
    "- ENDEREÇO: Localização da via onde o equipamento se encontra instalado. \n",
    "- SENTIDO: Sentido da via fiscalizado pelo equipamento. \n",
    "\n",
    "As colunas que serão usadas serão as seguintes:\n",
    "- \"ENDEREÇO\"\n",
    "- \"SENTIDO\"\n",
    "- \"FAIXA\"\n",
    "- \"CLASSIFICAÇÃO\"\n",
    "- \"TAMANHO\"\n",
    "- \"VELOCIDADE DA VIA\"\n",
    "- \"VELOCIDADE AFERIDA\"\n",
    "\n",
    "Os dados contínuos serão discretizados de forma arbitrária usando visualização de histogramas. E serão gerados dois novos atributos a partir da DATA HORA, VELOCIDADE DA VIA e VELOCIDADE AFERIDA:\n",
    "- \"PERIODO DIA\"\n",
    "- \"ULTRAPASSOU LIMITE\"\n",
    "Espera-se obter informações mais ricas usando dessa nova representação discreta dos horários, e minerar padrões em casos onde houveram infrações de trânsito.\n",
    "\n",
    "Os códigos abaixo consistem em uma dataclass usada para modelar o banco de dados usando `Polars` dataframes, e a chamada da classe para gerar arquivos CSV que serão usados na modelagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataLoader:\n",
    "    dir: str\n",
    "    target: str\n",
    "\n",
    "    def list_files(self):\n",
    "        months_folders = os.listdir(self.dir)\n",
    "        json_files_list = []\n",
    "\n",
    "        for m_folder in months_folders:\n",
    "            m_folder = os.path.join(self.dir, m_folder)\n",
    "            if os.path.isdir(m_folder):\n",
    "                d_folder_list = os.listdir(m_folder)\n",
    "                for d_folder in d_folder_list:\n",
    "                    d_folder = os.path.join(m_folder, d_folder)\n",
    "                    json_files = glob(f\"{d_folder}/*.json\")\n",
    "                    json_files_list.extend(json_files)\n",
    "\n",
    "        return json_files_list\n",
    "\n",
    "    def filter_location(self, p_df: pl.DataFrame):\n",
    "        filtered_df = (\n",
    "            p_df.lazy()\n",
    "            .filter(pl.col(\"ENDEREÇO\").str.contains(f\"{self.target}\"))\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        return filtered_df\n",
    "\n",
    "    def check_above_speed_limite(self, p_df: pl.DataFrame):\n",
    "        df = p_df.with_columns(\n",
    "            pl.col(\"VELOCIDADE AFERIDA\").cast(pl.Float32),\n",
    "            pl.col(\"VELOCIDADE DA VIA\").cast(pl.Float32),\n",
    "        )\n",
    "        df = df.with_columns(\n",
    "            (pl.col(\"VELOCIDADE AFERIDA\") > pl.col(\"VELOCIDADE DA VIA\")).alias(\n",
    "                \"ULTRAPASSOU LIMITE\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def categorize_time_of_day_extended(self, hour):\n",
    "        if 0 <= hour < 6:\n",
    "            return \"Madrugada\"\n",
    "        elif 6 <= hour < 9 or 17 <= hour < 20:\n",
    "            return \"Pico\"\n",
    "        elif 9 <= hour < 12:\n",
    "            return \"Manhã\"\n",
    "        elif 12 <= hour < 17:\n",
    "            return \"Tarde\"\n",
    "        else:\n",
    "            return \"Noite\"\n",
    "\n",
    "    def discretize_datetime(self, p_df: pl.DataFrame):\n",
    "        df = p_df.with_columns(\n",
    "            pl.col(\"DATA HORA\").str.strptime(pl.Datetime, format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "        )\n",
    "\n",
    "        df = (\n",
    "            df.lazy()\n",
    "            .with_columns(\n",
    "                pl.col(\"DATA HORA\")\n",
    "                .dt.hour()\n",
    "                .map_elements(\n",
    "                    self.categorize_time_of_day_extended, return_dtype=pl.String\n",
    "                )\n",
    "                .alias(\"PERIODO DIA\")\n",
    "            )\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def load_data(self) -> pl.DataFrame:\n",
    "        json_files_list = self.list_files()\n",
    "        dataframes_list = []\n",
    "\n",
    "        for file in tqdm(json_files_list):\n",
    "            df = pl.read_json(file)\n",
    "            filtered_df = self.filter_location(df)\n",
    "            filtered_df = self.check_above_speed_limite(filtered_df)\n",
    "\n",
    "            filtered_df = self.discretize_datetime(filtered_df)\n",
    "\n",
    "            filtered_df = filtered_df[\n",
    "                \"PERIODO DIA\",\n",
    "                \"ENDEREÇO\",\n",
    "                \"SENTIDO\",\n",
    "                \"FAIXA\",\n",
    "                \"CLASSIFICAÇÃO\",\n",
    "                \"TAMANHO\",\n",
    "                \"VELOCIDADE DA VIA\",\n",
    "                \"VELOCIDADE AFERIDA\",\n",
    "                \"ULTRAPASSOU LIMITE\",\n",
    "            ]\n",
    "            dataframes_list.append(filtered_df)\n",
    "\n",
    "        months_df = pl.concat(dataframes_list)\n",
    "\n",
    "        return months_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from dataLoader import DataLoader\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_dotenv(\"bhTrafficDataMining/humanAttempt/dataProcessing/config/.env\")\n",
    "    data_loader = DataLoader(\"bhTrafficDataMining/data\", os.environ[\"TARGET\"])\n",
    "    df = data_loader.load_data()\n",
    "    df.write_csv(\n",
    "        \"bhTrafficDataMining/humanAttempt/dataProcessing/dataProcessed/ABRIL_2022.csv\",\n",
    "    )\n",
    "    print(df[\"ENDEREÇO\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minerando Padrões em Casos de Infração de Trânsito"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando dos dados salvos em CSV, pode-se filtrar apenas os casos onde houveram infrações de trânsito. O Python conta com a biblioteca `mlxtend` que permite minerar padrões frequentes com algoritmos vistos em aula e gerar regras de associação. É possível selecionar qual métrica será usada para gerar as regras de associação entre os itemsets frequentes, no caso foi usada a confiança com um limiar arbitrário de 75%. A saída retornada pela biblioteca é um dataframe que contêm as regras de associação, ordenadas pelo suporte, contendo informações de várias métricas comumente usadas para avaliar regras de associação, dentre elas estão:\n",
    "\n",
    "- support(A->C) = support(A+C) [aka 'support'], range: [0, 1]\n",
    "- confidence(A->C) = support(A+C) / support(A), range: [0, 1]\n",
    "- lift(A->C) = confidence(A->C) / support(C), range: [0, inf]\n",
    "- leverage(A->C) = support(A->C) - support(A)*support(C), range: [-1, 1]\n",
    "- conviction = [1 - support(C)] / [1 - confidence(A->C)], range: [0, inf]\n",
    "- zhangs_metric(A->C) = leverage(A->C) / max(support(A->C)(1-support(A)), support(A)(support(C)-support(A->C))) range: [-1,1]\n",
    "\n",
    "Abaixo está o código da dataclass usada para preprocessar os dados, transformando os atributos citados anteriormente em one-hot-encoders para cada classe dos atributos. Em seguida está a chamada dessa classe, levando-os para a execução dos algoritmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataPreprocessor:\n",
    "    dir: str\n",
    "\n",
    "    def load_database(self, p_percentage) -> pl.DataFrame:\n",
    "        csv_files_list = glob(f\"{self.dir}/*.csv\")\n",
    "        percent_index = int(round(len(csv_files_list) * p_percentage))\n",
    "        dataframes_list = []\n",
    "        for file in tqdm(csv_files_list[:percent_index]):\n",
    "            df = pl.read_csv(file)\n",
    "            df = self.discretize_speed(df)\n",
    "            dataframes_list.append(df)\n",
    "\n",
    "        return pl.concat(dataframes_list)\n",
    "\n",
    "    def get_speep_intervals(self, p_speed):\n",
    "        if 0 <= p_speed < 50:\n",
    "            return \"velocidadeModerada\"\n",
    "        elif 50 <= p_speed < 100:\n",
    "            return \"velocidadeAlta\"\n",
    "        elif p_speed >= 100:\n",
    "            return \"velocidadeAltissima\"\n",
    "\n",
    "    def discretize_speed(self, p_df: pl.DataFrame):\n",
    "        df = (\n",
    "            p_df.lazy()\n",
    "            .with_columns(\n",
    "                pl.col(\"VELOCIDADE AFERIDA\")\n",
    "                .map_elements(self.get_speep_intervals, return_dtype=pl.String)\n",
    "                .alias(\"VELCOIDADE\")\n",
    "            )\n",
    "            .collect()\n",
    "        )\n",
    "\n",
    "        return df\n",
    "\n",
    "    def drop_undesired_columns(\n",
    "        self, p_df: pl.DataFrame, p_undesired_columns: list\n",
    "    ) -> pl.DataFrame:\n",
    "        return p_df.drop(p_undesired_columns)\n",
    "\n",
    "    def get_preprocessed_database(self, p_percentage: float, p_undesired_columns: list):\n",
    "        df = self.load_database(p_percentage)\n",
    "        df = self.drop_undesired_columns(df, p_undesired_columns)\n",
    "        df = df.to_dummies()\n",
    "        df = df.cast(pl.Boolean)\n",
    "\n",
    "        return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_preprocessor = DataPreprocessor(\n",
    "        \"bhTrafficDataMining/humanAttempt/dataProcessing/dataProcessed\"\n",
    "    )\n",
    "\n",
    "    df = data_preprocessor.get_preprocessed_database(\n",
    "        0.5, p_undesired_columns=[\"VELOCIDADE AFERIDA\", \"VELOCIDADE DA VIA\", \"TAMANHO\"]\n",
    "    )\n",
    "    print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from mlxtend.frequent_patterns import fpgrowth, association_rules\n",
    "from preprocessor import DataPreprocessor\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_preprocessor = DataPreprocessor(\n",
    "        \"bhTrafficDataMining/humanAttempt/dataProcessing/dataProcessed\"\n",
    "    )\n",
    "\n",
    "    df = data_preprocessor.get_preprocessed_database(\n",
    "        1.0, p_undesired_columns=[\"VELOCIDADE AFERIDA\", \"VELOCIDADE DA VIA\", \"TAMANHO\"]\n",
    "    )\n",
    "    df = df.lazy().filter(pl.col(\"ULTRAPASSOU LIMITE_true\") == True).collect()\n",
    "    df = df.drop([\"ULTRAPASSOU LIMITE_true\", \"ULTRAPASSOU LIMITE_false\"])\n",
    "    df = df.to_pandas()\n",
    "    print(df)\n",
    "\n",
    "    frequent_itemsets = fpgrowth(df, min_support=0.25, use_colnames=True, verbose=1)\n",
    "    print(frequent_itemsets)\n",
    "    frequent_itemsets.to_csv(\"bhTrafficDataMining/humanAttempt/resultados.csv\")\n",
    "\n",
    "    rules = association_rules(\n",
    "        frequent_itemsets,\n",
    "        metric=\"confidence\",\n",
    "        min_threshold=0.75,\n",
    "        num_itemsets=df.shape[0],\n",
    "    )\n",
    "    print(rules)\n",
    "    rules.to_csv(\"bhTrafficDataMining/humanAttempt/association_rules_conf75.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
